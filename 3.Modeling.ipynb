{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8120d9d7-9da4-4b61-8cfb-4feec19824c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Optuna (only for LGBM)\n",
    "import optuna\n",
    "\n",
    "ART_DIR = Path(\"artifacts_models\")\n",
    "ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_artifact(obj, name):\n",
    "    path = ART_DIR / name\n",
    "    joblib.dump(obj, path)\n",
    "    print(f\"Saved: {path}\")\n",
    "    return path\n",
    "\n",
    "def load_artifact(name):\n",
    "    path = ART_DIR / name\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Not found: {path}\")\n",
    "    obj = joblib.load(path)\n",
    "    print(f\"Loaded: {path}\")\n",
    "    return obj\n",
    "\n",
    "def sanitize_columns(cols):\n",
    "    cleaned = []\n",
    "    for c in cols:\n",
    "        c = str(c).replace(\"\\n\",\"_\").replace(\"\\r\",\"_\").replace(\"\\t\",\"_\")\n",
    "        c = re.sub(r'[{}\\[\\]\":,\\\\]', \"_\", c)\n",
    "        c = re.sub(r\"\\s+\",\"_\", c)\n",
    "        c = re.sub(r\"_+\",\"_\", c).strip(\"_\")\n",
    "        cleaned.append(c if c else \"feat\")\n",
    "\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in cleaned:\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}__{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def prepare_X_y_from_fs_df(fs_df: pd.DataFrame, target_col=\"TARGET\"):\n",
    "    df = fs_df.copy()\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"{target_col} not found\")\n",
    "\n",
    "    y = df[target_col].astype(int)\n",
    "    X = df.drop(columns=[target_col], errors=\"ignore\")\n",
    "\n",
    "    # fill\n",
    "    cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "    num_cols = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "    X[cat_cols] = X[cat_cols].fillna(\"Unknown\")\n",
    "    for c in num_cols:\n",
    "        X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "    # one-hot\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # bool -> int\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == bool:\n",
    "            X[c] = X[c].astype(np.int8)\n",
    "\n",
    "    # sanitize + inf cleanup\n",
    "    X.columns = sanitize_columns(X.columns)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def drop_correlated_features_train_only(X, y, threshold=0.95):\n",
    "    X = X.copy()\n",
    "\n",
    "    corr = X.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "\n",
    "    # target association (rank corr)\n",
    "    y_rank = pd.Series(y.values).rank().values\n",
    "    assoc = {}\n",
    "    for c in X.columns:\n",
    "        xr = pd.Series(X[c].values).rank().values\n",
    "        v = np.corrcoef(xr, y_rank)[0, 1]\n",
    "        assoc[c] = 0.0 if np.isnan(v) else abs(v)\n",
    "    assoc = pd.Series(assoc)\n",
    "\n",
    "    to_drop = set()\n",
    "    for col in upper.columns:\n",
    "        high = upper.index[upper[col] > threshold].tolist()\n",
    "        for row in high:\n",
    "            if row in to_drop or col in to_drop:\n",
    "                continue\n",
    "            drop = row if assoc[row] < assoc[col] else col\n",
    "            to_drop.add(drop)\n",
    "\n",
    "    keep_cols = [c for c in X.columns if c not in to_drop]\n",
    "    return X[keep_cols].copy(), sorted(list(to_drop))\n",
    "\n",
    "def lgbm_cv_oof_auc(X, y, params, n_splits=5, seed=42, verbose_eval=0):\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    models = []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "        y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        model = LGBMClassifier(**params, random_state=seed+fold, n_jobs=-1)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"auc\",\n",
    "            callbacks=[early_stopping(200, first_metric_only=True),\n",
    "                       log_evaluation(verbose_eval)]\n",
    "        )\n",
    "        oof[va] = model.predict_proba(X_va)[:, 1]\n",
    "        models.append(model)\n",
    "\n",
    "    return roc_auc_score(y, oof), oof, models\n",
    "\n",
    "def mean_gain_importance(lgbm_models):\n",
    "    gains = np.mean([m.booster_.feature_importance(importance_type=\"gain\") for m in lgbm_models], axis=0)\n",
    "    names = lgbm_models[0].booster_.feature_name()\n",
    "    return pd.Series(gains, index=names).sort_values(ascending=False)\n",
    "\n",
    "def xgb_train_cv_oof_auc_native(\n",
    "    X, y,\n",
    "    params,\n",
    "    num_boost_round=20000,\n",
    "    n_splits=5,\n",
    "    seed=42,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Native XGBoost training (xgb.train) with early stopping.\n",
    "    Works even when sklearn wrapper doesn't support early stopping/callbacks.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    boosters = []\n",
    "    best_iters = []\n",
    "\n",
    "    # Make sure params include required fields\n",
    "    xgb_params = dict(params)\n",
    "    xgb_params.setdefault(\"objective\", \"binary:logistic\")\n",
    "    xgb_params.setdefault(\"eval_metric\", \"auc\")\n",
    "    xgb_params.setdefault(\"tree_method\", \"hist\")\n",
    "    xgb_params.setdefault(\"seed\", seed)\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"\\n[XGB-NATIVE] Starting fold {fold}/{n_splits} ...\")\n",
    "\n",
    "        X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "        y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dvalid = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=[(dvalid, \"valid\")],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose_eval=verbose_eval\n",
    "        )\n",
    "\n",
    "        # predict using best_iteration\n",
    "        p_va = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
    "        oof[va] = p_va\n",
    "\n",
    "        fold_auc = roc_auc_score(y_va, p_va)\n",
    "        print(f\"[XGB-NATIVE] Fold {fold} AUC: {fold_auc:.5f} | best_iter: {booster.best_iteration}\")\n",
    "\n",
    "        boosters.append(booster)\n",
    "        best_iters.append(booster.best_iteration)\n",
    "\n",
    "    auc = roc_auc_score(y, oof)\n",
    "    print(f\"\\n[XGB-NATIVE] OOF AUC: {auc:.5f}\")\n",
    "\n",
    "    return auc, oof, boosters, best_iters\n",
    "\n",
    "\n",
    "def cv_auc_for_model_factory(X, y, model_factory, n_splits=5, seed=42):\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), start=1):\n",
    "        model = model_factory()\n",
    "        model.fit(X.iloc[tr], y.iloc[tr])\n",
    "        oof[va] = model.predict_proba(X.iloc[va])[:, 1]\n",
    "\n",
    "    return roc_auc_score(y, oof), oof\n",
    "\n",
    "def fit_full_and_save_model(algo_key, X, y, model, params_dict=None):\n",
    "    params_dict = params_dict or {}\n",
    "    bundle = {\n",
    "        \"algo\": algo_key,\n",
    "        \"params\": params_dict,\n",
    "        \"feature_names\": X.columns.tolist(),\n",
    "        \"model\": model\n",
    "    }\n",
    "    save_artifact(bundle, f\"model_{algo_key}.pkl\")\n",
    "    return bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "627ce5d0-7b7a-4c78-a10d-4d40cf270e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all: (307511, 239) pos_rate: 0.08072881945686496\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18446\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 233\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.770111\tvalid_0's binary_logloss: 0.552999\n",
      "[400]\tvalid_0's auc: 0.776544\tvalid_0's binary_logloss: 0.526421\n",
      "[600]\tvalid_0's auc: 0.777815\tvalid_0's binary_logloss: 0.508851\n",
      "[800]\tvalid_0's auc: 0.777904\tvalid_0's binary_logloss: 0.493714\n",
      "Early stopping, best iteration is:\n",
      "[677]\tvalid_0's auc: 0.778103\tvalid_0's binary_logloss: 0.502713\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18529\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.778803\tvalid_0's binary_logloss: 0.553033\n",
      "[400]\tvalid_0's auc: 0.786288\tvalid_0's binary_logloss: 0.52575\n",
      "[600]\tvalid_0's auc: 0.786996\tvalid_0's binary_logloss: 0.508114\n",
      "Early stopping, best iteration is:\n",
      "[584]\tvalid_0's auc: 0.787116\tvalid_0's binary_logloss: 0.509323\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18459\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.769842\tvalid_0's binary_logloss: 0.555283\n",
      "[400]\tvalid_0's auc: 0.777451\tvalid_0's binary_logloss: 0.528061\n",
      "[600]\tvalid_0's auc: 0.778546\tvalid_0's binary_logloss: 0.510528\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's auc: 0.778687\tvalid_0's binary_logloss: 0.51309\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18538\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 235\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.775055\tvalid_0's binary_logloss: 0.551581\n",
      "[400]\tvalid_0's auc: 0.781944\tvalid_0's binary_logloss: 0.524615\n",
      "[600]\tvalid_0's auc: 0.783432\tvalid_0's binary_logloss: 0.506634\n",
      "[800]\tvalid_0's auc: 0.783756\tvalid_0's binary_logloss: 0.491624\n",
      "[1000]\tvalid_0's auc: 0.783613\tvalid_0's binary_logloss: 0.478224\n",
      "Early stopping, best iteration is:\n",
      "[862]\tvalid_0's auc: 0.783787\tvalid_0's binary_logloss: 0.487339\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18451\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 234\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.768242\tvalid_0's binary_logloss: 0.554569\n",
      "[400]\tvalid_0's auc: 0.776541\tvalid_0's binary_logloss: 0.527674\n",
      "[600]\tvalid_0's auc: 0.778412\tvalid_0's binary_logloss: 0.50952\n",
      "[800]\tvalid_0's auc: 0.779117\tvalid_0's binary_logloss: 0.49424\n",
      "[1000]\tvalid_0's auc: 0.779218\tvalid_0's binary_logloss: 0.480816\n",
      "Early stopping, best iteration is:\n",
      "[916]\tvalid_0's auc: 0.779404\tvalid_0's binary_logloss: 0.486243\n",
      "Evaluated only: auc\n",
      "Baseline (importance run) OOF AUC: 0.7812267250802738\n",
      "Top40: (307511, 40) | After corr drop: (307511, 40) | Dropped: 0\n",
      "Saved: artifacts_models\\params_baseline_lgbm.pkl\n",
      "Saved: artifacts_models\\feature_selection_info.pkl\n",
      "Saved: artifacts_models\\data_final.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts_models/data_final.pkl')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_df = pd.read_csv(\"fs_df.csv\")\n",
    "X_all, y_all = prepare_X_y_from_fs_df(fs_df, target_col=\"TARGET\")\n",
    "print(\"X_all:\", X_all.shape, \"pos_rate:\", y_all.mean())\n",
    "\n",
    "# Baseline LGBM for importance \n",
    "baseline_lgbm_params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_samples=20,\n",
    "    class_weight=\"balanced\",\n",
    "    max_depth=-1,\n",
    ")\n",
    "\n",
    "auc_base_imp, oof_imp, models_imp = lgbm_cv_oof_auc(\n",
    "    X_all, y_all, baseline_lgbm_params,\n",
    "    n_splits=5, seed=42, verbose_eval=200\n",
    ")\n",
    "print(\"Baseline (importance run) OOF AUC:\", auc_base_imp)\n",
    "\n",
    "fi = mean_gain_importance(models_imp)\n",
    "top40 = fi.head(40).index.tolist()\n",
    "\n",
    "X_top40 = X_all[top40].copy()\n",
    "X_final, dropped_corr = drop_correlated_features_train_only(X_top40, y_all, threshold=0.95)\n",
    "\n",
    "print(\"Top40:\", X_top40.shape, \"| After corr drop:\", X_final.shape, \"| Dropped:\", len(dropped_corr))\n",
    "\n",
    "# Save artifacts to resume later\n",
    "save_artifact({\"baseline_lgbm_params\": baseline_lgbm_params}, \"params_baseline_lgbm.pkl\")\n",
    "save_artifact({\"importance_gain\": fi, \"top40_before_corr\": top40, \"dropped_corr\": dropped_corr}, \"feature_selection_info.pkl\")\n",
    "save_artifact({\"X_final\": X_final, \"y\": y_all}, \"data_final.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b16e02e4-8df3-4c49-8f64-61c278fbc82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts_models\\data_final.pkl\n",
      "Loaded: artifacts_models\\params_baseline_lgbm.pkl\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.767963\tvalid_0's binary_logloss: 0.556827\n",
      "[400]\tvalid_0's auc: 0.773999\tvalid_0's binary_logloss: 0.533266\n",
      "[600]\tvalid_0's auc: 0.774585\tvalid_0's binary_logloss: 0.517307\n",
      "[800]\tvalid_0's auc: 0.774543\tvalid_0's binary_logloss: 0.503629\n",
      "Early stopping, best iteration is:\n",
      "[771]\tvalid_0's auc: 0.774638\tvalid_0's binary_logloss: 0.505576\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.775962\tvalid_0's binary_logloss: 0.556828\n",
      "[400]\tvalid_0's auc: 0.781707\tvalid_0's binary_logloss: 0.533162\n",
      "[600]\tvalid_0's auc: 0.782442\tvalid_0's binary_logloss: 0.516845\n",
      "[800]\tvalid_0's auc: 0.782418\tvalid_0's binary_logloss: 0.502743\n",
      "Early stopping, best iteration is:\n",
      "[694]\tvalid_0's auc: 0.78271\tvalid_0's binary_logloss: 0.509894\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8970\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.766971\tvalid_0's binary_logloss: 0.559591\n",
      "[400]\tvalid_0's auc: 0.772777\tvalid_0's binary_logloss: 0.536015\n",
      "[600]\tvalid_0's auc: 0.77383\tvalid_0's binary_logloss: 0.519883\n",
      "[800]\tvalid_0's auc: 0.77388\tvalid_0's binary_logloss: 0.505888\n",
      "[1000]\tvalid_0's auc: 0.773552\tvalid_0's binary_logloss: 0.493493\n",
      "Early stopping, best iteration is:\n",
      "[810]\tvalid_0's auc: 0.773908\tvalid_0's binary_logloss: 0.505248\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8970\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.773116\tvalid_0's binary_logloss: 0.55532\n",
      "[400]\tvalid_0's auc: 0.779007\tvalid_0's binary_logloss: 0.531513\n",
      "[600]\tvalid_0's auc: 0.780101\tvalid_0's binary_logloss: 0.51559\n",
      "[800]\tvalid_0's auc: 0.78015\tvalid_0's binary_logloss: 0.501913\n",
      "[1000]\tvalid_0's auc: 0.780011\tvalid_0's binary_logloss: 0.489502\n",
      "Early stopping, best iteration is:\n",
      "[874]\tvalid_0's auc: 0.780325\tvalid_0's binary_logloss: 0.497168\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.764635\tvalid_0's binary_logloss: 0.557983\n",
      "[400]\tvalid_0's auc: 0.772172\tvalid_0's binary_logloss: 0.533824\n",
      "[600]\tvalid_0's auc: 0.773538\tvalid_0's binary_logloss: 0.517564\n",
      "[800]\tvalid_0's auc: 0.77399\tvalid_0's binary_logloss: 0.503453\n",
      "[1000]\tvalid_0's auc: 0.773939\tvalid_0's binary_logloss: 0.490702\n",
      "Early stopping, best iteration is:\n",
      "[863]\tvalid_0's auc: 0.774101\tvalid_0's binary_logloss: 0.499289\n",
      "Evaluated only: auc\n",
      "LGBM BASELINE OOF AUC: 0.7770541683081458\n",
      "[LightGBM] [Info] Number of positive: 24825, number of negative: 282686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 307511, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Saved: artifacts_models\\model_lgbm_base.pkl\n",
      "Saved: artifacts_models\\score_lgbm_base.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts_models/score_lgbm_base.pkl')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_artifact(\"data_final.pkl\")\n",
    "X_final = data[\"X_final\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "params = load_artifact(\"params_baseline_lgbm.pkl\")[\"baseline_lgbm_params\"]\n",
    "\n",
    "auc_lgbm_base, oof_lgbm_base, _ = lgbm_cv_oof_auc(X_final, y, params, n_splits=5, seed=42, verbose_eval=200)\n",
    "print(\"LGBM BASELINE OOF AUC:\", auc_lgbm_base)\n",
    "\n",
    "# Fit ONE model on full data + save\n",
    "lgbm_base_full = LGBMClassifier(**params, random_state=42, n_jobs=-1)\n",
    "lgbm_base_full.fit(X_final, y)\n",
    "\n",
    "bundle_lgbm_base = fit_full_and_save_model(\"lgbm_base\", X_final, y, lgbm_base_full, params_dict=params)\n",
    "save_artifact({\"auc\": auc_lgbm_base}, \"score_lgbm_base.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "994b5155-606a-47e7-9a94-256320df17e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 22:21:09,731] A new study created in memory with name: no-name-51f7654a-3f5f-4efe-8e74-04f7824f00ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts_models\\data_final.pkl\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8967\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2396]\tvalid_0's auc: 0.780257\tvalid_0's binary_logloss: 0.493304\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8968\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2331]\tvalid_0's auc: 0.778422\tvalid_0's binary_logloss: 0.498394\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4029]\tvalid_0's auc: 0.778775\tvalid_0's binary_logloss: 0.460321\n",
      "Evaluated only: auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 22:24:08,765] Trial 0 finished with value: 0.7787195390693188 and parameters: {'learning_rate': 0.005857924562213654, 'num_leaves': 84, 'min_child_samples': 49, 'subsample': 0.7448512995296056, 'colsample_bytree': 0.6458990912301835, 'reg_lambda': 1.9555965111371516, 'reg_alpha': 10.089532017033568}. Best is trial 0 with value: 0.7787195390693188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8967\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[400]\tvalid_0's auc: 0.777114\tvalid_0's binary_logloss: 0.502141\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8968\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[397]\tvalid_0's auc: 0.775928\tvalid_0's binary_logloss: 0.505306\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[512]\tvalid_0's auc: 0.775676\tvalid_0's binary_logloss: 0.487279\n",
      "Evaluated only: auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 22:24:43,180] Trial 1 finished with value: 0.7761269486221013 and parameters: {'learning_rate': 0.02742058420972762, 'num_leaves': 79, 'min_child_samples': 20, 'subsample': 0.8928167564854739, 'colsample_bytree': 0.6835246869646625, 'reg_lambda': 0.22246412247983582, 'reg_alpha': 0.0029679152685582974}. Best is trial 0 with value: 0.7787195390693188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8967\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3290]\tvalid_0's auc: 0.779527\tvalid_0's binary_logloss: 0.515793\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8968\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2204]\tvalid_0's auc: 0.777306\tvalid_0's binary_logloss: 0.53407\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3705]\tvalid_0's auc: 0.778104\tvalid_0's binary_logloss: 0.5106\n",
      "Evaluated only: auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 22:26:48,173] Trial 2 finished with value: 0.7781942956802982 and parameters: {'learning_rate': 0.00812870775431524, 'num_leaves': 28, 'min_child_samples': 80, 'subsample': 0.9333832041694925, 'colsample_bytree': 0.6608378687602444, 'reg_lambda': 0.05770604041590449, 'reg_alpha': 4.099812828082873}. Best is trial 0 with value: 0.7787195390693188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8967\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[800]\tvalid_0's auc: 0.777451\tvalid_0's binary_logloss: 0.491077\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8968\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[859]\tvalid_0's auc: 0.776158\tvalid_0's binary_logloss: 0.489121\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1115]\tvalid_0's auc: 0.775738\tvalid_0's binary_logloss: 0.468367\n",
      "Evaluated only: auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 22:28:10,164] Trial 3 finished with value: 0.7762354541403673 and parameters: {'learning_rate': 0.010440690620179154, 'num_leaves': 120, 'min_child_samples': 115, 'subsample': 0.9588937976584218, 'colsample_bytree': 0.9655217578665335, 'reg_lambda': 0.020213338905380462, 'reg_alpha': 1.1323203178433514}. Best is trial 0 with value: 0.7787195390693188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8967\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2472]\tvalid_0's auc: 0.778216\tvalid_0's binary_logloss: 0.519605\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188457\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8968\n",
      "[LightGBM] [Info] Number of data points in the train set: 205007, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2411]\tvalid_0's auc: 0.776189\tvalid_0's binary_logloss: 0.523837\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 16550, number of negative: 188458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 205008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3069]\tvalid_0's auc: 0.776922\tvalid_0's binary_logloss: 0.509939\n",
      "Evaluated only: auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 22:30:27,452] Trial 4 finished with value: 0.777042850892623 and parameters: {'learning_rate': 0.005932161628550322, 'num_leaves': 42, 'min_child_samples': 85, 'subsample': 0.7825608847549775, 'colsample_bytree': 0.8770945096147125, 'reg_lambda': 0.3977796049095145, 'reg_alpha': 0.00277632954633953}. Best is trial 0 with value: 0.7787195390693188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna best (3-fold) AUC: 0.7787195390693188\n",
      "Best params: {'n_estimators': 25000, 'class_weight': 'balanced', 'max_depth': -1, 'learning_rate': 0.005857924562213654, 'num_leaves': 84, 'min_child_samples': 49, 'subsample': 0.7448512995296056, 'colsample_bytree': 0.6458990912301835, 'reg_lambda': 1.9555965111371516, 'reg_alpha': 10.089532017033568}\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.755647\tvalid_0's binary_logloss: 0.597997\n",
      "[400]\tvalid_0's auc: 0.762418\tvalid_0's binary_logloss: 0.571861\n",
      "[600]\tvalid_0's auc: 0.767859\tvalid_0's binary_logloss: 0.557739\n",
      "[800]\tvalid_0's auc: 0.771328\tvalid_0's binary_logloss: 0.54773\n",
      "[1000]\tvalid_0's auc: 0.773718\tvalid_0's binary_logloss: 0.539596\n",
      "[1200]\tvalid_0's auc: 0.775215\tvalid_0's binary_logloss: 0.532627\n",
      "[1400]\tvalid_0's auc: 0.776138\tvalid_0's binary_logloss: 0.526604\n",
      "[1600]\tvalid_0's auc: 0.776781\tvalid_0's binary_logloss: 0.521069\n",
      "[1800]\tvalid_0's auc: 0.777111\tvalid_0's binary_logloss: 0.516142\n",
      "[2000]\tvalid_0's auc: 0.777362\tvalid_0's binary_logloss: 0.511541\n",
      "[2200]\tvalid_0's auc: 0.777549\tvalid_0's binary_logloss: 0.50714\n",
      "[2400]\tvalid_0's auc: 0.777633\tvalid_0's binary_logloss: 0.502993\n",
      "[2600]\tvalid_0's auc: 0.777623\tvalid_0's binary_logloss: 0.499012\n",
      "[2800]\tvalid_0's auc: 0.777646\tvalid_0's binary_logloss: 0.495149\n",
      "[3000]\tvalid_0's auc: 0.777706\tvalid_0's binary_logloss: 0.491337\n",
      "[3200]\tvalid_0's auc: 0.777738\tvalid_0's binary_logloss: 0.487696\n",
      "[3400]\tvalid_0's auc: 0.77776\tvalid_0's binary_logloss: 0.484185\n",
      "[3600]\tvalid_0's auc: 0.777821\tvalid_0's binary_logloss: 0.480718\n",
      "Early stopping, best iteration is:\n",
      "[3595]\tvalid_0's auc: 0.777836\tvalid_0's binary_logloss: 0.480812\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.760251\tvalid_0's binary_logloss: 0.599626\n",
      "[400]\tvalid_0's auc: 0.76873\tvalid_0's binary_logloss: 0.572984\n",
      "[600]\tvalid_0's auc: 0.775084\tvalid_0's binary_logloss: 0.558199\n",
      "[800]\tvalid_0's auc: 0.778959\tvalid_0's binary_logloss: 0.547839\n",
      "[1000]\tvalid_0's auc: 0.781373\tvalid_0's binary_logloss: 0.539557\n",
      "[1200]\tvalid_0's auc: 0.782861\tvalid_0's binary_logloss: 0.532534\n",
      "[1400]\tvalid_0's auc: 0.78391\tvalid_0's binary_logloss: 0.526261\n",
      "[1600]\tvalid_0's auc: 0.784476\tvalid_0's binary_logloss: 0.520806\n",
      "[1800]\tvalid_0's auc: 0.784783\tvalid_0's binary_logloss: 0.515926\n",
      "[2000]\tvalid_0's auc: 0.784879\tvalid_0's binary_logloss: 0.511356\n",
      "[2200]\tvalid_0's auc: 0.784984\tvalid_0's binary_logloss: 0.507025\n",
      "[2400]\tvalid_0's auc: 0.785113\tvalid_0's binary_logloss: 0.50273\n",
      "[2600]\tvalid_0's auc: 0.785206\tvalid_0's binary_logloss: 0.498718\n",
      "[2800]\tvalid_0's auc: 0.78517\tvalid_0's binary_logloss: 0.49477\n",
      "Early stopping, best iteration is:\n",
      "[2693]\tvalid_0's auc: 0.785251\tvalid_0's binary_logloss: 0.496826\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8970\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.753606\tvalid_0's binary_logloss: 0.600427\n",
      "[400]\tvalid_0's auc: 0.760798\tvalid_0's binary_logloss: 0.574824\n",
      "[600]\tvalid_0's auc: 0.766294\tvalid_0's binary_logloss: 0.560759\n",
      "[800]\tvalid_0's auc: 0.769827\tvalid_0's binary_logloss: 0.550637\n",
      "[1000]\tvalid_0's auc: 0.772376\tvalid_0's binary_logloss: 0.5424\n",
      "[1200]\tvalid_0's auc: 0.773964\tvalid_0's binary_logloss: 0.535411\n",
      "[1400]\tvalid_0's auc: 0.774854\tvalid_0's binary_logloss: 0.529293\n",
      "[1600]\tvalid_0's auc: 0.77537\tvalid_0's binary_logloss: 0.52405\n",
      "[1800]\tvalid_0's auc: 0.775675\tvalid_0's binary_logloss: 0.519219\n",
      "[2000]\tvalid_0's auc: 0.775943\tvalid_0's binary_logloss: 0.51469\n",
      "[2200]\tvalid_0's auc: 0.776055\tvalid_0's binary_logloss: 0.510388\n",
      "[2400]\tvalid_0's auc: 0.776107\tvalid_0's binary_logloss: 0.506251\n",
      "[2600]\tvalid_0's auc: 0.776182\tvalid_0's binary_logloss: 0.502283\n",
      "Early stopping, best iteration is:\n",
      "[2536]\tvalid_0's auc: 0.776208\tvalid_0's binary_logloss: 0.503531\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8970\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.759626\tvalid_0's binary_logloss: 0.598109\n",
      "[400]\tvalid_0's auc: 0.767441\tvalid_0's binary_logloss: 0.571138\n",
      "[600]\tvalid_0's auc: 0.773309\tvalid_0's binary_logloss: 0.556368\n",
      "[800]\tvalid_0's auc: 0.776798\tvalid_0's binary_logloss: 0.545949\n",
      "[1000]\tvalid_0's auc: 0.779113\tvalid_0's binary_logloss: 0.537768\n",
      "[1200]\tvalid_0's auc: 0.780477\tvalid_0's binary_logloss: 0.530838\n",
      "[1400]\tvalid_0's auc: 0.78143\tvalid_0's binary_logloss: 0.524748\n",
      "[1600]\tvalid_0's auc: 0.781971\tvalid_0's binary_logloss: 0.519376\n",
      "[1800]\tvalid_0's auc: 0.782303\tvalid_0's binary_logloss: 0.51454\n",
      "[2000]\tvalid_0's auc: 0.782437\tvalid_0's binary_logloss: 0.510119\n",
      "[2200]\tvalid_0's auc: 0.782594\tvalid_0's binary_logloss: 0.505806\n",
      "[2400]\tvalid_0's auc: 0.782634\tvalid_0's binary_logloss: 0.501677\n",
      "Early stopping, best iteration is:\n",
      "[2275]\tvalid_0's auc: 0.782668\tvalid_0's binary_logloss: 0.50421\n",
      "Evaluated only: auc\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226149\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8965\n",
      "[LightGBM] [Info] Number of data points in the train set: 246009, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.750268\tvalid_0's binary_logloss: 0.599312\n",
      "[400]\tvalid_0's auc: 0.758095\tvalid_0's binary_logloss: 0.573511\n",
      "[600]\tvalid_0's auc: 0.764349\tvalid_0's binary_logloss: 0.559231\n",
      "[800]\tvalid_0's auc: 0.768536\tvalid_0's binary_logloss: 0.548837\n",
      "[1000]\tvalid_0's auc: 0.771071\tvalid_0's binary_logloss: 0.540711\n",
      "[1200]\tvalid_0's auc: 0.772846\tvalid_0's binary_logloss: 0.533674\n",
      "[1400]\tvalid_0's auc: 0.77405\tvalid_0's binary_logloss: 0.527554\n",
      "[1600]\tvalid_0's auc: 0.774851\tvalid_0's binary_logloss: 0.522071\n",
      "[1800]\tvalid_0's auc: 0.775301\tvalid_0's binary_logloss: 0.517268\n",
      "[2000]\tvalid_0's auc: 0.775623\tvalid_0's binary_logloss: 0.512607\n",
      "[2200]\tvalid_0's auc: 0.775689\tvalid_0's binary_logloss: 0.508265\n",
      "[2400]\tvalid_0's auc: 0.775872\tvalid_0's binary_logloss: 0.504024\n",
      "[2600]\tvalid_0's auc: 0.775883\tvalid_0's binary_logloss: 0.499988\n",
      "Early stopping, best iteration is:\n",
      "[2463]\tvalid_0's auc: 0.775921\tvalid_0's binary_logloss: 0.502681\n",
      "Evaluated only: auc\n",
      "LGBM OPT OOF AUC: 0.7794456447105429\n",
      "[LightGBM] [Info] Number of positive: 24825, number of negative: 282686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 307511, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Saved: artifacts_models\\model_lgbm_opt.pkl\n",
      "Saved: artifacts_models\\params_lgbm_opt.pkl\n",
      "Saved: artifacts_models\\score_lgbm_opt.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts_models/score_lgbm_opt.pkl')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_artifact(\"data_final.pkl\")\n",
    "X_final = data[\"X_final\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "def optuna_tune_lgbm_fast(X, y, n_trials=5, n_splits=3, seed=42):\n",
    "    def objective(trial):\n",
    "        params = dict(\n",
    "            n_estimators=15000,\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=True),\n",
    "            num_leaves=trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "            min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 120),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-3, 20.0, log=True),\n",
    "            reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-3, 20.0, log=True),\n",
    "            class_weight=\"balanced\",\n",
    "            max_depth=-1,\n",
    "        )\n",
    "        auc, _, _ = lgbm_cv_oof_auc(X, y, params, n_splits=n_splits, seed=seed, verbose_eval=0)\n",
    "        return auc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = dict(\n",
    "        n_estimators=25000,\n",
    "        class_weight=\"balanced\",\n",
    "        max_depth=-1,\n",
    "        **study.best_params\n",
    "    )\n",
    "    return study, best_params\n",
    "\n",
    "study, best_lgbm_params = optuna_tune_lgbm_fast(X_final, y, n_trials=5, n_splits=3, seed=42)\n",
    "print(\"Optuna best (3-fold) AUC:\", study.best_value)\n",
    "print(\"Best params:\", best_lgbm_params)\n",
    "\n",
    "# Final eval 5-fold\n",
    "auc_lgbm_opt, oof_lgbm_opt, _ = lgbm_cv_oof_auc(X_final, y, best_lgbm_params, n_splits=5, seed=42, verbose_eval=200)\n",
    "print(\"LGBM OPT OOF AUC:\", auc_lgbm_opt)\n",
    "\n",
    "# Fit ONE model on full data + save\n",
    "lgbm_opt_full = LGBMClassifier(**best_lgbm_params, random_state=42, n_jobs=-1)\n",
    "lgbm_opt_full.fit(X_final, y)\n",
    "\n",
    "bundle_lgbm_opt = fit_full_and_save_model(\"lgbm_opt\", X_final, y, lgbm_opt_full, params_dict=best_lgbm_params)\n",
    "\n",
    "save_artifact({\"best_params\": best_lgbm_params}, \"params_lgbm_opt.pkl\")\n",
    "save_artifact({\"auc\": auc_lgbm_opt}, \"score_lgbm_opt.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd73f465-201c-4fc3-808e-d75f7e717d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts_models\\data_final.pkl\n",
      "\n",
      "[XGB-NATIVE] Starting fold 1/5 ...\n",
      "[0]\tvalid-auc:0.70893\n",
      "[200]\tvalid-auc:0.76441\n",
      "[400]\tvalid-auc:0.77043\n",
      "[600]\tvalid-auc:0.77353\n",
      "[800]\tvalid-auc:0.77507\n",
      "[1000]\tvalid-auc:0.77598\n",
      "[1200]\tvalid-auc:0.77649\n",
      "[1400]\tvalid-auc:0.77709\n",
      "[1600]\tvalid-auc:0.77702\n",
      "[1674]\tvalid-auc:0.77706\n",
      "[XGB-NATIVE] Fold 1 AUC: 0.77714 | best_iter: 1474\n",
      "\n",
      "[XGB-NATIVE] Starting fold 2/5 ...\n",
      "[0]\tvalid-auc:0.71830\n",
      "[200]\tvalid-auc:0.77244\n",
      "[400]\tvalid-auc:0.77900\n",
      "[600]\tvalid-auc:0.78218\n",
      "[800]\tvalid-auc:0.78371\n",
      "[1000]\tvalid-auc:0.78472\n",
      "[1200]\tvalid-auc:0.78536\n",
      "[1400]\tvalid-auc:0.78577\n",
      "[1527]\tvalid-auc:0.78574\n",
      "[XGB-NATIVE] Fold 2 AUC: 0.78587 | best_iter: 1327\n",
      "\n",
      "[XGB-NATIVE] Starting fold 3/5 ...\n",
      "[0]\tvalid-auc:0.70677\n",
      "[200]\tvalid-auc:0.76307\n",
      "[400]\tvalid-auc:0.76958\n",
      "[600]\tvalid-auc:0.77239\n",
      "[800]\tvalid-auc:0.77381\n",
      "[1000]\tvalid-auc:0.77471\n",
      "[1200]\tvalid-auc:0.77516\n",
      "[1400]\tvalid-auc:0.77539\n",
      "[1600]\tvalid-auc:0.77525\n",
      "[1621]\tvalid-auc:0.77528\n",
      "[XGB-NATIVE] Fold 3 AUC: 0.77543 | best_iter: 1421\n",
      "\n",
      "[XGB-NATIVE] Starting fold 4/5 ...\n",
      "[0]\tvalid-auc:0.71557\n",
      "[200]\tvalid-auc:0.76968\n",
      "[400]\tvalid-auc:0.77616\n",
      "[600]\tvalid-auc:0.77890\n",
      "[800]\tvalid-auc:0.78034\n",
      "[1000]\tvalid-auc:0.78142\n",
      "[1200]\tvalid-auc:0.78192\n",
      "[1400]\tvalid-auc:0.78222\n",
      "[1600]\tvalid-auc:0.78236\n",
      "[1800]\tvalid-auc:0.78247\n",
      "[2000]\tvalid-auc:0.78263\n",
      "[2200]\tvalid-auc:0.78266\n",
      "[2400]\tvalid-auc:0.78270\n",
      "[2517]\tvalid-auc:0.78258\n",
      "[XGB-NATIVE] Fold 4 AUC: 0.78278 | best_iter: 2317\n",
      "\n",
      "[XGB-NATIVE] Starting fold 5/5 ...\n",
      "[0]\tvalid-auc:0.70237\n",
      "[200]\tvalid-auc:0.76110\n",
      "[400]\tvalid-auc:0.76852\n",
      "[600]\tvalid-auc:0.77228\n",
      "[800]\tvalid-auc:0.77433\n",
      "[1000]\tvalid-auc:0.77559\n",
      "[1200]\tvalid-auc:0.77637\n",
      "[1400]\tvalid-auc:0.77670\n",
      "[1600]\tvalid-auc:0.77685\n",
      "[1800]\tvalid-auc:0.77700\n",
      "[2000]\tvalid-auc:0.77681\n",
      "[2044]\tvalid-auc:0.77687\n",
      "[XGB-NATIVE] Fold 5 AUC: 0.77707 | best_iter: 1844\n",
      "\n",
      "[XGB-NATIVE] OOF AUC: 0.77961\n",
      "XGB BASELINE (NATIVE) OOF AUC: 0.7796101314081729\n",
      "Saved: artifacts_models\\score_xgb_base.pkl\n",
      "XGB full-fit num_boost_round: 1475\n",
      "Saved: artifacts_models\\model_xgb_base.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts_models/model_xgb_base.pkl')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_artifact(\"data_final.pkl\")\n",
    "X_final = data[\"X_final\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "# Baseline XGB params (native)\n",
    "xgb_params_native = dict(\n",
    "    eta=0.03,                 # learning_rate\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    lambda_=1.0,              # reg_lambda in native\n",
    "    alpha=0.0                 # reg_alpha in native\n",
    ")\n",
    "\n",
    "auc_xgb_base, oof_xgb_base, xgb_boosters, best_iters = xgb_train_cv_oof_auc_native(\n",
    "    X_final, y,\n",
    "    params=xgb_params_native,\n",
    "    num_boost_round=20000,\n",
    "    n_splits=5,\n",
    "    seed=42,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=200\n",
    ")\n",
    "\n",
    "print(\"XGB BASELINE (NATIVE) OOF AUC:\", auc_xgb_base)\n",
    "\n",
    "# Save score + best iters\n",
    "save_artifact({\"auc\": auc_xgb_base, \"best_iters_cv\": best_iters, \"params_native\": xgb_params_native}, \"score_xgb_base.pkl\")\n",
    "\n",
    "# Fit ONE FULL booster with n_estimators = median(best_iter)+1\n",
    "n_rounds_full = int(np.median(best_iters)) + 1\n",
    "print(\"XGB full-fit num_boost_round:\", n_rounds_full)\n",
    "\n",
    "dall = xgb.DMatrix(X_final, label=y)\n",
    "\n",
    "booster_full = xgb.train(\n",
    "    params={**xgb_params_native, \"objective\": \"binary:logistic\", \"eval_metric\": \"auc\", \"tree_method\": \"hist\", \"seed\": 42},\n",
    "    dtrain=dall,\n",
    "    num_boost_round=n_rounds_full,\n",
    "    evals=[]\n",
    ")\n",
    "\n",
    "# Save full model bundle (note: booster is saved directly)\n",
    "bundle_xgb = {\n",
    "    \"algo\": \"xgb_base_native\",\n",
    "    \"params\": {**xgb_params_native, \"num_boost_round\": n_rounds_full},\n",
    "    \"feature_names\": X_final.columns.tolist(),\n",
    "    \"model\": booster_full\n",
    "}\n",
    "save_artifact(bundle_xgb, \"model_xgb_base.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bbd71ae-cf2c-4fb6-b8ce-72f9ae055e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts_models\\data_final.pkl\n",
      "LogReg OOF AUC: 0.5941517914335778\n",
      "Saved: artifacts_models\\model_logreg.pkl\n",
      "Saved: artifacts_models\\score_logreg.pkl\n",
      "GaussianNB OOF AUC: 0.6120036678218704\n",
      "Saved: artifacts_models\\model_gnb.pkl\n",
      "Saved: artifacts_models\\score_gnb.pkl\n",
      "AdaBoost OOF AUC: 0.7412783426380111\n",
      "Saved: artifacts_models\\model_ada.pkl\n",
      "Saved: artifacts_models\\score_ada.pkl\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.88 MiB for an array with shape (246009,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\_utils.py\", line 109, in __call__\n    return self.func(**kwargs)\n           ~~~~~~~~~^^^^^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 184, in __call__\n    return self.function(*args, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 186, in _parallel_build_trees\n    curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n                          ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 191, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 208, in compute_sample_weight\n    compute_class_weight(\n    ~~~~~~~~~~~~~~~~~~~~^\n        class_weight_k, classes=classes_subsample, y=y_subsample\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ),\n    ^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 191, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 75, in compute_class_weight\n    y_ind = le.fit_transform(y)\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py\", line 121, in fit_transform\n    self.classes_, y = _unique(y, return_inverse=True)\n                       ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\_encode.py\", line 50, in _unique\n    return _unique_np(\n        values, return_inverse=return_inverse, return_counts=return_counts\n    )\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\_encode.py\", line 65, in _unique_np\n    uniques, inverse = xp.unique_inverse(values)\n                       ~~~~~~~~~~~~~~~~~^^^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\_internal.py\", line 34, in wrapped_f\n    return f(*args, xp=xp, **kwargs)\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\common\\_aliases.py\", line 240, in unique_inverse\n    values, inverse_indices = xp.unique(\n                              ~~~~~~~~~^\n        x,\n        ^^\n    ...<3 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py\", line 296, in unique\n    ret = _unique1d(ar, return_index, return_inverse, return_counts,\n                    equal_nan=equal_nan, inverse_shape=ar.shape, axis=None,\n                    sorted=sorted)\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py\", line 407, in _unique1d\n    imask = np.cumsum(mask) - 1\n            ~~~~~~~~~^^^^^^\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py\", line 2914, in cumsum\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n  File \"C:\\Users\\CASPER\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py\", line 54, in _wrapfunc\n    return bound(*args, **kwds)\nnumpy._core._exceptions._ArrayMemoryError: Unable to allocate 1.88 MiB for an array with shape (246009,) and data type int64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m scores[\u001b[33m\"\u001b[39m\u001b[33mada\u001b[39m\u001b[33m\"\u001b[39m] = auc_ada\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# RandomForest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m auc_rf, _ = \u001b[43mcv_auc_for_model_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbalanced_subsample\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRandomForest OOF AUC:\u001b[39m\u001b[33m\"\u001b[39m, auc_rf)\n\u001b[32m     58\u001b[39m rf_full = RandomForestClassifier(\n\u001b[32m     59\u001b[39m     n_estimators=\u001b[32m600\u001b[39m, max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     60\u001b[39m     min_samples_leaf=\u001b[32m2\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m     61\u001b[39m     class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     62\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mcv_auc_for_model_factory\u001b[39m\u001b[34m(X, y, model_factory, n_splits, seed)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold, (tr, va) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf.split(X, y), start=\u001b[32m1\u001b[39m):\n\u001b[32m    221\u001b[39m     model = model_factory()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     oof[va] = model.predict_proba(X.iloc[va])[:, \u001b[32m1\u001b[39m]\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m roc_auc_score(y, oof), oof\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.88 MiB for an array with shape (246009,) and data type int64"
     ]
    }
   ],
   "source": [
    "data = load_artifact(\"data_final.pkl\")\n",
    "X_final = data[\"X_final\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "scores = {}\n",
    "\n",
    "# Logistic Regression\n",
    "auc_logreg, _ = cv_auc_for_model_factory(\n",
    "    X_final, y,\n",
    "    model_factory=lambda: LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\", random_state=42),\n",
    "    n_splits=5, seed=42\n",
    ")\n",
    "print(\"LogReg OOF AUC:\", auc_logreg)\n",
    "logreg_full = LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\", random_state=42)\n",
    "logreg_full.fit(X_final, y)\n",
    "fit_full_and_save_model(\"logreg\", X_final, y, logreg_full, params_dict={})\n",
    "save_artifact({\"auc\": auc_logreg}, \"score_logreg.pkl\")\n",
    "scores[\"logreg\"] = auc_logreg\n",
    "\n",
    "# GaussianNB\n",
    "auc_gnb, _ = cv_auc_for_model_factory(\n",
    "    X_final, y,\n",
    "    model_factory=lambda: GaussianNB(),\n",
    "    n_splits=5, seed=42\n",
    ")\n",
    "print(\"GaussianNB OOF AUC:\", auc_gnb)\n",
    "gnb_full = GaussianNB()\n",
    "gnb_full.fit(X_final, y)\n",
    "fit_full_and_save_model(\"gnb\", X_final, y, gnb_full, params_dict={})\n",
    "save_artifact({\"auc\": auc_gnb}, \"score_gnb.pkl\")\n",
    "scores[\"gnb\"] = auc_gnb\n",
    "\n",
    "# AdaBoost\n",
    "auc_ada, _ = cv_auc_for_model_factory(\n",
    "    X_final, y,\n",
    "    model_factory=lambda: AdaBoostClassifier(n_estimators=300, learning_rate=0.05, random_state=42),\n",
    "    n_splits=5, seed=42\n",
    ")\n",
    "print(\"AdaBoost OOF AUC:\", auc_ada)\n",
    "ada_full = AdaBoostClassifier(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "ada_full.fit(X_final, y)\n",
    "fit_full_and_save_model(\"ada\", X_final, y, ada_full, params_dict={})\n",
    "save_artifact({\"auc\": auc_ada}, \"score_ada.pkl\")\n",
    "scores[\"ada\"] = auc_ada\n",
    "\n",
    "# RandomForest\n",
    "#auc_rf, _ = cv_auc_for_model_factory(\n",
    "#    X_final, y,\n",
    "#    model_factory=lambda: RandomForestClassifier(\n",
    "#        n_estimators=600, max_depth=None,\n",
    "#        min_samples_leaf=2, n_jobs=-1,\n",
    "#        class_weight=\"balanced_subsample\",\n",
    "#        random_state=42\n",
    "#    ),\n",
    "#    n_splits=5, seed=42\n",
    "#)\n",
    "#print(\"RandomForest OOF AUC:\", auc_rf)\n",
    "#rf_full = RandomForestClassifier(\n",
    "#    n_estimators=600, max_depth=None,\n",
    "#    min_samples_leaf=2, n_jobs=-1,\n",
    "#    class_weight=\"balanced_subsample\",\n",
    "#    random_state=42\n",
    "#)\n",
    "#rf_full.fit(X_final, y)\n",
    "#fit_full_and_save_model(\"rf\", X_final, y, rf_full, params_dict={})\n",
    "#save_artifact({\"auc\": auc_rf}, \"score_rf.pkl\")\n",
    "#scores[\"rf\"] = auc_rf\n",
    "\n",
    "save_artifact({\"scores_partial\": scores}, \"scores_other_models.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3065b98d-310f-4e32-9097-3fba6afb495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts_models\\scores_other_models.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts_models/scores_other_models.pkl')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_artifact({\"scores_partial\": scores}, \"scores_other_models.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "311d115f-c524-4937-b78e-b4816372579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts_models\\score_lgbm_base.pkl\n",
      "Loaded: artifacts_models\\score_lgbm_opt.pkl\n",
      "Loaded: artifacts_models\\score_xgb_base.pkl\n",
      "Loaded: artifacts_models\\scores_other_models.pkl\n",
      "=== AUC RANKING ===\n",
      "xgb_base  : 0.779610\n",
      "lgbm_opt  : 0.779446\n",
      "lgbm_base : 0.777054\n",
      "ada       : 0.741278\n",
      "gnb       : 0.612004\n",
      "logreg    : 0.594152\n",
      "\n",
      "CHAMPION: xgb_base\n",
      "Loaded: artifacts_models\\model_xgb_base.pkl\n",
      "Saved: artifacts_models\\champion_model.pkl\n",
      "Saved: artifacts_models\\model_registry.pkl\n",
      "\n",
      "Saved champion_model.pkl and model_registry.pkl in artifacts_models/\n"
     ]
    }
   ],
   "source": [
    "# Load scores\n",
    "scores = {}\n",
    "scores[\"lgbm_base\"] = load_artifact(\"score_lgbm_base.pkl\")[\"auc\"]\n",
    "scores[\"lgbm_opt\"]  = load_artifact(\"score_lgbm_opt.pkl\")[\"auc\"]\n",
    "scores[\"xgb_base\"]  = load_artifact(\"score_xgb_base.pkl\")[\"auc\"]\n",
    "\n",
    "other = load_artifact(\"scores_other_models.pkl\")[\"scores_partial\"]\n",
    "scores.update(other)\n",
    "\n",
    "# Rank\n",
    "ranked = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"=== AUC RANKING ===\")\n",
    "for k, v in ranked:\n",
    "    print(f\"{k:10s}: {v:.6f}\")\n",
    "\n",
    "champion_key = ranked[0][0]\n",
    "print(\"\\nCHAMPION:\", champion_key)\n",
    "\n",
    "# Load champion bundle and save as champion_model.pkl (copy)\n",
    "champ_bundle = load_artifact(f\"model_{champion_key}.pkl\")\n",
    "save_artifact(champ_bundle, \"champion_model.pkl\")\n",
    "\n",
    "# Also save an index of all models saved\n",
    "all_models_saved = [p.name for p in ART_DIR.glob(\"model_*.pkl\")]\n",
    "save_artifact({\"scores\": scores, \"champion\": champion_key, \"model_files\": all_models_saved}, \"model_registry.pkl\")\n",
    "\n",
    "print(\"\\nSaved champion_model.pkl and model_registry.pkl in artifacts_models/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
