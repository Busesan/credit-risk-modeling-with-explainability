{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52eb5d91-97ef-4ce2-a93f-69a7d2c892a9",
   "metadata": {},
   "source": [
    "# 1. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7759e16e-89cb-4c7d-9395-568c27652881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged feature table: (307511, 188)\n",
      "X shape: (307511, 316) y shape: (307511,)\n",
      "NaN left in X: 0\n",
      "Target rate: 0.08072881945686496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "\n",
    "DATA_PATH = Path(\"home-credit-default-risk\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) LOAD DATA\n",
    "# -----------------------\n",
    "def load_home_credit_data(data_path: Path):\n",
    "    files = {\n",
    "        \"application_train\": \"application_train.csv\",\n",
    "        \"bureau\": \"bureau.csv\",\n",
    "        \"bureau_balance\": \"bureau_balance.csv\",\n",
    "        \"credit_card_balance\": \"credit_card_balance.csv\",\n",
    "        \"installments_payments\": \"installments_payments.csv\",\n",
    "        \"POS_CASH_balance\": \"POS_CASH_balance.csv\",\n",
    "        \"previous_application\": \"previous_application.csv\",\n",
    "    }\n",
    "    dfs = {k: pd.read_csv(data_path / v) for k, v in files.items()}\n",
    "    return dfs\n",
    "\n",
    "# -----------------------\n",
    "# 2) HELPERS\n",
    "# -----------------------\n",
    "def add_missing_flags(df, cols, suffix=\"_MISSING\"):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c + suffix] = df[c].isna().astype(np.int8)\n",
    "    return df\n",
    "\n",
    "def fix_application_anomalies(df):\n",
    "    \"\"\"\n",
    "    Known Home Credit anomalies:\n",
    "    - DAYS_EMPLOYED == 365243 is a sentinel (unknown). Replace with NaN + flag.\n",
    "    - DAYS_BIRTH is negative days; create AGE_YEARS.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"DAYS_EMPLOYED\" in df.columns:\n",
    "        df[\"DAYS_EMPLOYED_ANOM\"] = (df[\"DAYS_EMPLOYED\"] == 365243).astype(np.int8)\n",
    "        df.loc[df[\"DAYS_EMPLOYED\"] == 365243, \"DAYS_EMPLOYED\"] = np.nan\n",
    "\n",
    "    if \"DAYS_BIRTH\" in df.columns:\n",
    "        df[\"AGE_YEARS\"] = (-df[\"DAYS_BIRTH\"] / 365.25).astype(float)\n",
    "\n",
    "    # Some people also turn DAYS_EMPLOYED into years\n",
    "    if \"DAYS_EMPLOYED\" in df.columns:\n",
    "        df[\"EMPLOYED_YEARS\"] = (-df[\"DAYS_EMPLOYED\"] / 365.25).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def safe_ratio(a, b):\n",
    "    \"\"\"Avoid division by zero.\"\"\"\n",
    "    return np.where((b == 0) | pd.isna(b), np.nan, a / b)\n",
    "\n",
    "# -----------------------\n",
    "# 3) AGGREGATIONS (SK_ID_CURR LEVEL)\n",
    "# -----------------------\n",
    "def aggregate_bureau(bureau: pd.DataFrame):\n",
    "    b = bureau.copy()\n",
    "\n",
    "    # simple flags\n",
    "    b[\"CREDIT_ACTIVE_IS_ACTIVE\"] = (b[\"CREDIT_ACTIVE\"] == \"Active\").astype(int)\n",
    "    b[\"CREDIT_ACTIVE_IS_CLOSED\"] = (b[\"CREDIT_ACTIVE\"] == \"Closed\").astype(int)\n",
    "\n",
    "    agg = b.groupby(\"SK_ID_CURR\").agg(\n",
    "        bureau_loan_count=(\"SK_ID_BUREAU\", \"count\"),\n",
    "        bureau_active_loans=(\"CREDIT_ACTIVE_IS_ACTIVE\", \"sum\"),\n",
    "        bureau_closed_loans=(\"CREDIT_ACTIVE_IS_CLOSED\", \"sum\"),\n",
    "        bureau_total_credit_sum=(\"AMT_CREDIT_SUM\", \"sum\"),\n",
    "        bureau_total_credit_mean=(\"AMT_CREDIT_SUM\", \"mean\"),\n",
    "        bureau_debt_sum=(\"AMT_CREDIT_SUM_DEBT\", \"sum\"),\n",
    "        bureau_debt_mean=(\"AMT_CREDIT_SUM_DEBT\", \"mean\"),\n",
    "        bureau_overdue_sum=(\"AMT_CREDIT_SUM_OVERDUE\", \"sum\"),\n",
    "        bureau_overdue_mean=(\"AMT_CREDIT_SUM_OVERDUE\", \"mean\"),\n",
    "        bureau_days_credit_mean=(\"DAYS_CREDIT\", \"mean\"),\n",
    "        bureau_days_enddate_mean=(\"DAYS_CREDIT_ENDDATE\", \"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    return agg\n",
    "\n",
    "def aggregate_bureau_balance(bureau_balance: pd.DataFrame, bureau: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    bureau_balance is keyed by SK_ID_BUREAU, not SK_ID_CURR.\n",
    "    So: aggregate bureau_balance per SK_ID_BUREAU -> merge to bureau -> aggregate to SK_ID_CURR.\n",
    "    \"\"\"\n",
    "    bb = bureau_balance.copy()\n",
    "\n",
    "    # status distributions (0,1,2,3,4,5,C,X)\n",
    "    # We'll compute counts + ratios for \"bad\" statuses\n",
    "    bad_status = {\"1\", \"2\", \"3\", \"4\", \"5\"}\n",
    "    bb[\"BB_IS_BAD\"] = bb[\"STATUS\"].isin(bad_status).astype(int)\n",
    "\n",
    "    bb_agg_bureau = bb.groupby(\"SK_ID_BUREAU\").agg(\n",
    "        bb_months_count=(\"MONTHS_BALANCE\", \"count\"),\n",
    "        bb_bad_months=(\"BB_IS_BAD\", \"sum\"),\n",
    "        bb_latest_month=(\"MONTHS_BALANCE\", \"max\"),\n",
    "        bb_oldest_month=(\"MONTHS_BALANCE\", \"min\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # merge to bureau to get SK_ID_CURR\n",
    "    b = bureau[[\"SK_ID_BUREAU\", \"SK_ID_CURR\"]].merge(bb_agg_bureau, on=\"SK_ID_BUREAU\", how=\"left\")\n",
    "\n",
    "    # aggregate to customer\n",
    "    agg = b.groupby(\"SK_ID_CURR\").agg(\n",
    "        bb_total_months=(\"bb_months_count\", \"sum\"),\n",
    "        bb_total_bad_months=(\"bb_bad_months\", \"sum\"),\n",
    "        bb_avg_latest_month=(\"bb_latest_month\", \"mean\"),\n",
    "        bb_avg_oldest_month=(\"bb_oldest_month\", \"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # add ratio\n",
    "    agg[\"bb_bad_ratio\"] = safe_ratio(agg[\"bb_total_bad_months\"], agg[\"bb_total_months\"])\n",
    "\n",
    "    return agg\n",
    "\n",
    "def aggregate_previous_application(prev: pd.DataFrame):\n",
    "    p = prev.copy()\n",
    "\n",
    "    # contract status flags\n",
    "    p[\"PREV_APPROVED\"] = (p[\"NAME_CONTRACT_STATUS\"] == \"Approved\").astype(int)\n",
    "    p[\"PREV_REFUSED\"] = (p[\"NAME_CONTRACT_STATUS\"] == \"Refused\").astype(int)\n",
    "\n",
    "    # time features: DAYS_DECISION (negative days)\n",
    "    agg = p.groupby(\"SK_ID_CURR\").agg(\n",
    "        prev_app_count=(\"SK_ID_PREV\", \"count\"),\n",
    "        prev_approved=(\"PREV_APPROVED\", \"sum\"),\n",
    "        prev_refused=(\"PREV_REFUSED\", \"sum\"),\n",
    "        prev_credit_mean=(\"AMT_CREDIT\", \"mean\"),\n",
    "        prev_credit_sum=(\"AMT_CREDIT\", \"sum\"),\n",
    "        prev_annuity_mean=(\"AMT_ANNUITY\", \"mean\"),\n",
    "        prev_goods_price_mean=(\"AMT_GOODS_PRICE\", \"mean\"),\n",
    "        prev_days_decision_mean=(\"DAYS_DECISION\", \"mean\"),\n",
    "        prev_days_decision_min=(\"DAYS_DECISION\", \"min\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    agg[\"prev_approval_rate\"] = safe_ratio(agg[\"prev_approved\"], agg[\"prev_app_count\"])\n",
    "    return agg\n",
    "\n",
    "def aggregate_installments(inst: pd.DataFrame):\n",
    "    i = inst.copy()\n",
    "\n",
    "    # Payment behavior\n",
    "    i[\"PAYMENT_DIFF\"] = i[\"AMT_PAYMENT\"] - i[\"AMT_INSTALMENT\"]\n",
    "    i[\"LATE\"] = (i[\"DAYS_ENTRY_PAYMENT\"] > i[\"DAYS_INSTALMENT\"]).astype(int)\n",
    "    i[\"DAYS_LATE\"] = (i[\"DAYS_ENTRY_PAYMENT\"] - i[\"DAYS_INSTALMENT\"]).clip(lower=0)\n",
    "\n",
    "    agg = i.groupby(\"SK_ID_CURR\").agg(\n",
    "        inst_count=(\"SK_ID_PREV\", \"count\"),\n",
    "        inst_late_ratio=(\"LATE\", \"mean\"),\n",
    "        inst_days_late_mean=(\"DAYS_LATE\", \"mean\"),\n",
    "        inst_days_late_max=(\"DAYS_LATE\", \"max\"),\n",
    "        inst_payment_diff_mean=(\"PAYMENT_DIFF\", \"mean\"),\n",
    "        inst_payment_mean=(\"AMT_PAYMENT\", \"mean\"),\n",
    "        inst_instalment_mean=(\"AMT_INSTALMENT\", \"mean\"),\n",
    "        inst_payment_sum=(\"AMT_PAYMENT\", \"sum\"),\n",
    "        inst_instalment_sum=(\"AMT_INSTALMENT\", \"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    agg[\"inst_paid_ratio_sum\"] = safe_ratio(agg[\"inst_payment_sum\"], agg[\"inst_instalment_sum\"])\n",
    "    return agg\n",
    "\n",
    "def aggregate_pos_cash(pos: pd.DataFrame):\n",
    "    p = pos.copy()\n",
    "\n",
    "    p[\"POS_LATE\"] = (p[\"SK_DPD\"] > 0).astype(int)\n",
    "\n",
    "    agg = p.groupby(\"SK_ID_CURR\").agg(\n",
    "        pos_records=(\"SK_ID_PREV\", \"count\"),\n",
    "        pos_unique_prev=(\"SK_ID_PREV\", \"nunique\"),\n",
    "        pos_late_ratio=(\"POS_LATE\", \"mean\"),\n",
    "        pos_skdpd_mean=(\"SK_DPD\", \"mean\"),\n",
    "        pos_skdpd_max=(\"SK_DPD\", \"max\"),\n",
    "        pos_months_balance_min=(\"MONTHS_BALANCE\", \"min\"),\n",
    "        pos_months_balance_max=(\"MONTHS_BALANCE\", \"max\"),\n",
    "    ).reset_index()\n",
    "    return agg\n",
    "\n",
    "def aggregate_credit_card(cc: pd.DataFrame):\n",
    "    c = cc.copy()\n",
    "\n",
    "    # utilization proxy: balance / limit (limit can be 0 or missing)\n",
    "    if \"AMT_CREDIT_LIMIT_ACTUAL\" in c.columns and \"AMT_BALANCE\" in c.columns:\n",
    "        c[\"CC_UTIL\"] = safe_ratio(c[\"AMT_BALANCE\"], c[\"AMT_CREDIT_LIMIT_ACTUAL\"])\n",
    "    else:\n",
    "        c[\"CC_UTIL\"] = np.nan\n",
    "\n",
    "    agg = c.groupby(\"SK_ID_CURR\").agg(\n",
    "        cc_records=(\"SK_ID_PREV\", \"count\"),\n",
    "        cc_unique_prev=(\"SK_ID_PREV\", \"nunique\"),\n",
    "        cc_balance_mean=(\"AMT_BALANCE\", \"mean\"),\n",
    "        cc_balance_max=(\"AMT_BALANCE\", \"max\"),\n",
    "        cc_limit_mean=(\"AMT_CREDIT_LIMIT_ACTUAL\", \"mean\"),\n",
    "        cc_util_mean=(\"CC_UTIL\", \"mean\"),\n",
    "        cc_skdpd_mean=(\"SK_DPD\", \"mean\") if \"SK_DPD\" in c.columns else (\"CC_UTIL\", \"mean\"),\n",
    "        cc_skdpd_max=(\"SK_DPD\", \"max\") if \"SK_DPD\" in c.columns else (\"CC_UTIL\", \"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    return agg\n",
    "\n",
    "# -----------------------\n",
    "# 4) MERGE ALL FEATURES\n",
    "# -----------------------\n",
    "def build_feature_table(dfs):\n",
    "    app = dfs[\"application_train\"].copy()\n",
    "    app = fix_application_anomalies(app)\n",
    "\n",
    "    # simple ratios in app\n",
    "    for col in [\"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_INCOME_TOTAL\"]:\n",
    "        if col not in app.columns:\n",
    "            raise ValueError(f\"Missing expected column in application_train: {col}\")\n",
    "\n",
    "    app[\"CREDIT_INCOME_RATIO\"] = safe_ratio(app[\"AMT_CREDIT\"], app[\"AMT_INCOME_TOTAL\"])\n",
    "    app[\"ANNUITY_INCOME_RATIO\"] = safe_ratio(app[\"AMT_ANNUITY\"], app[\"AMT_INCOME_TOTAL\"])\n",
    "    app[\"CREDIT_TERM\"] = safe_ratio(app[\"AMT_CREDIT\"], app[\"AMT_ANNUITY\"])\n",
    "\n",
    "    # Aggregations\n",
    "    bureau_agg = aggregate_bureau(dfs[\"bureau\"])\n",
    "    bb_agg = aggregate_bureau_balance(dfs[\"bureau_balance\"], dfs[\"bureau\"])\n",
    "    prev_agg = aggregate_previous_application(dfs[\"previous_application\"])\n",
    "    inst_agg = aggregate_installments(dfs[\"installments_payments\"])\n",
    "    pos_agg = aggregate_pos_cash(dfs[\"POS_CASH_balance\"])\n",
    "    cc_agg = aggregate_credit_card(dfs[\"credit_card_balance\"])\n",
    "\n",
    "    # Merge into app\n",
    "    out = app.merge(bureau_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    out = out.merge(bb_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    out = out.merge(prev_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    out = out.merge(inst_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    out = out.merge(pos_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    out = out.merge(cc_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "    # Missing flags for strategic columns (value + missingness both can be predictive)\n",
    "    strategic = [c for c in [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\",\n",
    "                            \"bureau_debt_sum\", \"bb_bad_ratio\",\n",
    "                            \"prev_approval_rate\", \"inst_late_ratio\",\n",
    "                            \"cc_util_mean\", \"pos_late_ratio\"] if c in out.columns]\n",
    "    out = add_missing_flags(out, strategic)\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# 5) CLEANING + ENCODING\n",
    "# -----------------------\n",
    "def clean_and_encode(full_df, target_col=\"TARGET\", drop_cols=(\"SK_ID_CURR\",), fill_cat=\"Unknown\"):\n",
    "    df = full_df.copy()\n",
    "\n",
    "    # Identify columns\n",
    "    cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "    num_cols = df.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "    # Fill categorical\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].fillna(fill_cat)\n",
    "\n",
    "    # Fill numeric (median), excluding target\n",
    "    if target_col in df.columns:\n",
    "        num_cols_wo_target = [c for c in num_cols if c != target_col]\n",
    "    else:\n",
    "        num_cols_wo_target = num_cols\n",
    "\n",
    "    medians = {}\n",
    "    for c in num_cols_wo_target:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            med = df[c].median()\n",
    "            medians[c] = med\n",
    "            df[c] = df[c].fillna(med)\n",
    "\n",
    "    # Build X/y\n",
    "    y = df[target_col].astype(int) if target_col in df.columns else None\n",
    "    X = df.drop(columns=[target_col], errors=\"ignore\")\n",
    "\n",
    "    # drop IDs\n",
    "    for c in drop_cols:\n",
    "        if c in X.columns:\n",
    "            X = X.drop(columns=[c])\n",
    "\n",
    "    # One-hot encode\n",
    "    X = pd.get_dummies(X, dummy_na=False)\n",
    "\n",
    "    return X, y, df, medians\n",
    "\n",
    "# -----------------------\n",
    "# 6) RUN PIPELINE\n",
    "# -----------------------\n",
    "dfs = load_home_credit_data(DATA_PATH)\n",
    "\n",
    "full_features = build_feature_table(dfs)\n",
    "print(\"Merged feature table:\", full_features.shape)\n",
    "\n",
    "X, y, cleaned_df, medians = clean_and_encode(full_features)\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# Sanity checks\n",
    "print(\"NaN left in X:\", int(X.isna().any().sum()))\n",
    "print(\"Target rate:\", y.mean())\n",
    "\n",
    "# Optional: save\n",
    "# cleaned_df.to_csv(\"home_credit_cleaned_with_aggs.csv\", index=False)\n",
    "# X.to_parquet(\"home_credit_X.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107119be-f800-47ce-b9e7-6cb00a568221",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa3002-0155-4add-8427-4b5b649fca27",
   "metadata": {},
   "source": [
    "# 2.a. Application Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4cbf5a-ba4d-47e2-9f6a-2383d9343986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_app_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Basic ratios (some already in your pipeline, keep safe)\n",
    "    if {\"AMT_CREDIT\",\"AMT_INCOME_TOTAL\"}.issubset(df.columns):\n",
    "        df[\"FE_CREDIT_INCOME_RATIO\"] = df[\"AMT_CREDIT\"] / df[\"AMT_INCOME_TOTAL\"].replace(0, np.nan)\n",
    "\n",
    "    if {\"AMT_ANNUITY\",\"AMT_INCOME_TOTAL\"}.issubset(df.columns):\n",
    "        df[\"FE_ANNUITY_INCOME_RATIO\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"].replace(0, np.nan)\n",
    "\n",
    "    if {\"AMT_CREDIT\",\"AMT_ANNUITY\"}.issubset(df.columns):\n",
    "        df[\"FE_CREDIT_TERM\"] = df[\"AMT_CREDIT\"] / df[\"AMT_ANNUITY\"].replace(0, np.nan)\n",
    "\n",
    "    if {\"AMT_GOODS_PRICE\",\"AMT_CREDIT\"}.issubset(df.columns):\n",
    "        df[\"FE_GOODS_CREDIT_RATIO\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"].replace(0, np.nan)\n",
    "\n",
    "    # --- Family / children normalization\n",
    "    if {\"CNT_FAM_MEMBERS\",\"AMT_INCOME_TOTAL\"}.issubset(df.columns):\n",
    "        df[\"FE_INCOME_PER_FAMILY\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"].replace(0, np.nan)\n",
    "\n",
    "    if {\"CNT_CHILDREN\",\"CNT_FAM_MEMBERS\"}.issubset(df.columns):\n",
    "        df[\"FE_CHILDREN_RATIO\"] = df[\"CNT_CHILDREN\"] / df[\"CNT_FAM_MEMBERS\"].replace(0, np.nan)\n",
    "\n",
    "    # --- Document / flag counts (these are weak individually, stronger as totals)\n",
    "    doc_cols = [c for c in df.columns if c.startswith(\"FLAG_DOC\")]\n",
    "    if doc_cols:\n",
    "        df[\"FE_FLAG_DOC_SUM\"] = df[doc_cols].sum(axis=1)\n",
    "\n",
    "    flag_cols = [c for c in df.columns if c.startswith(\"FLAG_\") and c not in doc_cols]\n",
    "    # Some FLAG_* are binary; sum can act as \"how many flags\"\n",
    "    if flag_cols:\n",
    "        # keep it bounded: ensure numeric\n",
    "        df[\"FE_FLAG_SUM\"] = df[flag_cols].select_dtypes(exclude=\"object\").sum(axis=1)\n",
    "\n",
    "    # --- External sources: mean / min / max / std + missing count\n",
    "    ext = [c for c in [\"EXT_SOURCE_1\",\"EXT_SOURCE_2\",\"EXT_SOURCE_3\"] if c in df.columns]\n",
    "    if ext:\n",
    "        df[\"FE_EXT_MEAN\"] = df[ext].mean(axis=1)\n",
    "        df[\"FE_EXT_MIN\"] = df[ext].min(axis=1)\n",
    "        df[\"FE_EXT_MAX\"] = df[ext].max(axis=1)\n",
    "        df[\"FE_EXT_STD\"] = df[ext].std(axis=1)\n",
    "        df[\"FE_EXT_MISSING_COUNT\"] = df[ext].isna().sum(axis=1)\n",
    "\n",
    "    # --- Age / employment interactions (if you created AGE_YEARS / EMPLOYED_YEARS)\n",
    "    if {\"AGE_YEARS\",\"EMPLOYED_YEARS\"}.issubset(df.columns):\n",
    "        df[\"FE_EMPLOYED_AGE_RATIO\"] = df[\"EMPLOYED_YEARS\"] / df[\"AGE_YEARS\"].replace(0, np.nan)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575aa254-674f-4071-95d9-5cd51f03580e",
   "metadata": {},
   "source": [
    "# 2.b. Recent Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50b647c-1c79-400e-a6a4-a8ce5cae13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recent_installment_features(inst: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    installments_payments: DAYS_INSTALMENT negative (past).\n",
    "    We'll define \"recent window\" by DAYS_INSTALMENT >= -365 (last 12 months), >= -180 (last 6 months).\n",
    "    Returns SK_ID_CURR-level features.\n",
    "    \"\"\"\n",
    "    i = inst.copy()\n",
    "\n",
    "    i[\"LATE\"] = (i[\"DAYS_ENTRY_PAYMENT\"] > i[\"DAYS_INSTALMENT\"]).astype(int)\n",
    "    i[\"DAYS_LATE\"] = (i[\"DAYS_ENTRY_PAYMENT\"] - i[\"DAYS_INSTALMENT\"]).clip(lower=0)\n",
    "\n",
    "    feats = []\n",
    "    for window, name in [(365, \"12M\"), (180, \"6M\")]:\n",
    "        recent = i[i[\"DAYS_INSTALMENT\"] >= -window].copy()\n",
    "        agg = recent.groupby(\"SK_ID_CURR\").agg(\n",
    "            **{\n",
    "                f\"FE_INST_LATE_RATIO_{name}\": (\"LATE\", \"mean\"),\n",
    "                f\"FE_INST_DAYS_LATE_MEAN_{name}\": (\"DAYS_LATE\", \"mean\"),\n",
    "                f\"FE_INST_COUNT_{name}\": (\"SK_ID_PREV\", \"count\"),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        feats.append(agg)\n",
    "\n",
    "    # merge the two windows together\n",
    "    out = feats[0]\n",
    "    out = out.merge(feats[1], on=\"SK_ID_CURR\", how=\"outer\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac5294-2cda-4006-95a0-3e2ece8c014f",
   "metadata": {},
   "source": [
    "# POS: SK-DPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f9bfe8-4955-4212-bc83-4d36ec198045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recent_pos_features(pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = pos.copy()\n",
    "    p[\"POS_LATE\"] = (p[\"SK_DPD\"] > 0).astype(int)\n",
    "\n",
    "    feats = []\n",
    "    for window, name in [(12, \"12M\"), (6, \"6M\")]:\n",
    "        # MONTHS_BALANCE: 0 is current, -1 previous month, etc.\n",
    "        recent = p[p[\"MONTHS_BALANCE\"] >= -window].copy()\n",
    "        agg = recent.groupby(\"SK_ID_CURR\").agg(\n",
    "            **{\n",
    "                f\"FE_POS_LATE_RATIO_{name}\": (\"POS_LATE\", \"mean\"),\n",
    "                f\"FE_POS_SKDPD_MEAN_{name}\": (\"SK_DPD\", \"mean\"),\n",
    "                f\"FE_POS_COUNT_{name}\": (\"SK_ID_PREV\", \"count\"),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        feats.append(agg)\n",
    "\n",
    "    out = feats[0].merge(feats[1], on=\"SK_ID_CURR\", how=\"outer\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ba61c-daa5-4401-bda7-2fee92296b7a",
   "metadata": {},
   "source": [
    "# Credit Card: utilization / balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94953726-2723-40da-ada0-0cda3e9513cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recent_cc_features(cc: pd.DataFrame) -> pd.DataFrame:\n",
    "    c = cc.copy()\n",
    "\n",
    "    if {\"AMT_BALANCE\",\"AMT_CREDIT_LIMIT_ACTUAL\"}.issubset(c.columns):\n",
    "        c[\"CC_UTIL\"] = c[\"AMT_BALANCE\"] / c[\"AMT_CREDIT_LIMIT_ACTUAL\"].replace(0, np.nan)\n",
    "    else:\n",
    "        c[\"CC_UTIL\"] = np.nan\n",
    "\n",
    "    feats = []\n",
    "    for window, name in [(12, \"12M\"), (6, \"6M\")]:\n",
    "        recent = c[c[\"MONTHS_BALANCE\"] >= -window].copy()\n",
    "        agg = recent.groupby(\"SK_ID_CURR\").agg(\n",
    "            **{\n",
    "                f\"FE_CC_UTIL_MEAN_{name}\": (\"CC_UTIL\", \"mean\"),\n",
    "                f\"FE_CC_BAL_MEAN_{name}\": (\"AMT_BALANCE\", \"mean\"),\n",
    "                f\"FE_CC_COUNT_{name}\": (\"SK_ID_PREV\", \"count\"),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        feats.append(agg)\n",
    "\n",
    "    out = feats[0].merge(feats[1], on=\"SK_ID_CURR\", how=\"outer\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf36e9-a50e-426b-b804-4f4fd9f003f0",
   "metadata": {},
   "source": [
    "# 2.c. Group-based normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51113426-7b7e-4af9-985c-c26bcbc48216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_zscores(df: pd.DataFrame, group_col: str, value_cols: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if group_col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    for v in value_cols:\n",
    "        if v not in df.columns:\n",
    "            continue\n",
    "        grp_mean = df.groupby(group_col)[v].transform(\"mean\")\n",
    "        grp_std = df.groupby(group_col)[v].transform(\"std\")\n",
    "        df[f\"FE_{v}_Z_IN_{group_col}\"] = (df[v] - grp_mean) / grp_std.replace(0, np.nan)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8d4a92-c999-4d43-9c14-9a1b6ae7e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes: (307511, 386) (307511,)\n",
      "NaN left in X: 0\n",
      "Target rate: 0.08072881945686496\n"
     ]
    }
   ],
   "source": [
    "# --- 1) full_features \n",
    "full_features = build_feature_table(dfs)\n",
    "\n",
    "# --- 2) app-level FE ---\n",
    "full_features = add_app_features(full_features)\n",
    "\n",
    "# --- 3) recent behavior FE from raw tables ---\n",
    "inst_recent = add_recent_installment_features(dfs[\"installments_payments\"])\n",
    "pos_recent  = add_recent_pos_features(dfs[\"POS_CASH_balance\"])\n",
    "cc_recent   = add_recent_cc_features(dfs[\"credit_card_balance\"])\n",
    "\n",
    "full_features = full_features.merge(inst_recent, on=\"SK_ID_CURR\", how=\"left\")\n",
    "full_features = full_features.merge(pos_recent,  on=\"SK_ID_CURR\", how=\"left\")\n",
    "full_features = full_features.merge(cc_recent,   on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "# --- 4) group-based z-scores  ---\n",
    "full_features = add_group_zscores(\n",
    "    full_features,\n",
    "    group_col=\"ORGANIZATION_TYPE\",\n",
    "    value_cols=[c for c in [\"AMT_INCOME_TOTAL\",\"AMT_CREDIT\",\"AMT_ANNUITY\"] if c in full_features.columns]\n",
    ")\n",
    "\n",
    "# --- 5) Add missing flags for the new engineered features too (keep signal)\n",
    "engineered_cols = [c for c in full_features.columns if c.startswith(\"FE_\")]\n",
    "full_features = add_missing_flags(full_features, engineered_cols[:60])  # limit: too many flags can explode\n",
    "\n",
    "# --- 6) Clean + encode into X/y ---\n",
    "X, y, cleaned_df, medians = clean_and_encode(full_features)\n",
    "\n",
    "print(\"Final shapes:\", X.shape, y.shape)\n",
    "print(\"NaN left in X:\", int(X.isna().any().sum()))\n",
    "print(\"Target rate:\", y.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae7491-9fde-4fd8-b8b1-f49563f33a4d",
   "metadata": {},
   "source": [
    "# 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8934e3a-52fc-499e-b97d-f99b0a7f45b7",
   "metadata": {},
   "source": [
    "# 3.a. Hard filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456cd8b8-7b85-45de-b91c-c5253eb3ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_features(df, target_col=\"TARGET\"):\n",
    "    drop_cols = []\n",
    "\n",
    "    # ID-like columns\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"SK_ID\"):\n",
    "            drop_cols.append(c)\n",
    "\n",
    "    # Constant columns\n",
    "    nunique = df.nunique()\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    drop_cols.extend(const_cols)\n",
    "\n",
    "    # Explicitly known useless flags (domain-based)\n",
    "    known_useless = [c for c in df.columns if c.startswith(\"FLAG_DOCUMENT\")]\n",
    "    drop_cols.extend(known_useless)\n",
    "\n",
    "    drop_cols = list(set(drop_cols))\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    return df, drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7dbd6f-c1cf-4de6-b0f9-676372d3f10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped in stage 1: 27\n"
     ]
    }
   ],
   "source": [
    "fs_df = full_features.copy()\n",
    "fs_df, dropped_stage1 = drop_useless_features(fs_df)\n",
    "\n",
    "print(\"Dropped in stage 1:\", len(dropped_stage1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c85b2-976c-498e-8a4b-4f3849988ea3",
   "metadata": {},
   "source": [
    "# 3.b. Low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcef6e68-9d0a-4c2e-8883-fccc8339b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_variance(df, threshold=0.99):\n",
    "    drop_cols = []\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            continue\n",
    "        top_freq = df[c].value_counts(normalize=True, dropna=False).iloc[0]\n",
    "        if top_freq >= threshold:\n",
    "            drop_cols.append(c)\n",
    "\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    return df, drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9e7b69-df90-4153-9438-253142bc8004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped low-variance: 12\n"
     ]
    }
   ],
   "source": [
    "fs_df, dropped_low_var = drop_low_variance(fs_df, threshold=0.99)\n",
    "print(\"Dropped low-variance:\", len(dropped_low_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f8ab6-97e2-4a16-8d74-3dd4e69c9e9a",
   "metadata": {},
   "source": [
    "# 3.b.1. redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69da3a7-2762-4fc9-8ac4-56f9bf5ddfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_highly_correlated(df, threshold=0.95):\n",
    "    num_df = df.select_dtypes(exclude=\"object\").copy()\n",
    "    corr = num_df.corr().abs()\n",
    "\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "\n",
    "    drop_cols = [\n",
    "        column for column in upper.columns\n",
    "        if any(upper[column] > threshold)\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    return df, drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "090553d4-3ecf-41b7-9d88-9daf8c44cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped correlated: 62\n"
     ]
    }
   ],
   "source": [
    "fs_df, dropped_corr = drop_highly_correlated(fs_df, threshold=0.95)\n",
    "print(\"Dropped correlated:\", len(dropped_corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329a431-3600-475e-9097-a03cf0742fb5",
   "metadata": {},
   "source": [
    "# 3.c. Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71fe5050-be44-4d94-bf63-7328cc13ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_numeric_by_target_corr(df, target_col=\"TARGET\", min_abs_corr=0.01):\n",
    "    num_cols = df.select_dtypes(exclude=\"object\").columns\n",
    "    num_cols = [c for c in num_cols if c != target_col]\n",
    "\n",
    "    corr = df[num_cols + [target_col]].corr(method=\"spearman\")[target_col]\n",
    "    corr = corr.drop(target_col)\n",
    "\n",
    "    keep = corr[abs(corr) >= min_abs_corr].index.tolist()\n",
    "    drop = corr[abs(corr) < min_abs_corr].index.tolist()\n",
    "\n",
    "    return keep, drop, corr.sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d65a5dc-2faa-49ca-bacd-d92cdc9837db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped weak numeric: 0\n"
     ]
    }
   ],
   "source": [
    "keep_num, drop_num, num_corr = select_numeric_by_target_corr(\n",
    "    fs_df, min_abs_corr=0.01\n",
    ")\n",
    "\n",
    "fs_df = fs_df.drop(columns=drop_num)\n",
    "print(\"Dropped weak numeric:\", len(drop_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011460d8-5cdd-40a2-b445-2e80a40c837a",
   "metadata": {},
   "source": [
    "# 3.c.1. Default rate variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a835ff22-4cf0-42ec-b8bc-ead5758fabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_categorical_by_target_spread(df, target_col=\"TARGET\", min_std=0.005):\n",
    "    cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "    keep, drop = [], []\n",
    "    for c in cat_cols:\n",
    "        rates = df.groupby(c)[target_col].mean()\n",
    "        if rates.std() >= min_std:\n",
    "            keep.append(c)\n",
    "        else:\n",
    "            drop.append(c)\n",
    "\n",
    "    return keep, drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50809e19-6f37-4bb9-9843-7f2f26e2234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped weak categorical: 2\n"
     ]
    }
   ],
   "source": [
    "keep_cat, drop_cat = select_categorical_by_target_spread(fs_df)\n",
    "\n",
    "fs_df = fs_df.drop(columns=drop_cat)\n",
    "print(\"Dropped weak categorical:\", len(drop_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "651f4849-7918-4ef3-8d37-3c23ba96425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features: (307511, 131)\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining features:\", fs_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d4ab95-46bd-455b-8352-2408523d48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_df.to_csv(\"fs_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
